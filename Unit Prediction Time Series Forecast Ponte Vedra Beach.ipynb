{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95409a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8423b9d-cbd1-442f-91b9-bfacac5859d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('Trip List Edited 12.1.2022-2.1.2023.csv', parse_dates=['Date of Service'], index_col = 'Date of Service')\n",
    "data_2 = pd.read_csv('Trip List Edited 2.1.2023-4.1.2023.csv', parse_dates=['Date of Service'], index_col = 'Date of Service')\n",
    "data_3 = pd.read_csv('Trip List Edited 4.1.2023-6.1.2023.csv', parse_dates=['Date of Service'], index_col = 'Date of Service')\n",
    "data_4 = pd.read_csv('Trip List Edited 8.1.2023-12.21-2023.csv', parse_dates=['Date of Service'], index_col = 'Date of Service')\n",
    "data_current_cycle = pd.read_csv('Trip List Edited Current.csv', parse_dates=['Date of Service'], index_col = 'Date of Service')\n",
    "\n",
    "#data = pd.concat([data_1, data_2, data_3, data_4])\n",
    "data = pd.concat([data_1, data_2, data_3, data_4, data_current_cycle])\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#     print(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Date of Service'] = pd.to_datetime(data['Date of Service'])\n",
    "#data.set_index('Date of Service')\n",
    "#data.sort_index()\n",
    "data['Location'] = data['Location'].str.strip()\n",
    "\n",
    "# Filtering for Jacksonville location\n",
    "data_location = data[data['Location'] == 'PONTE VEDRA BEACH']\n",
    "\n",
    "# Parameters for the model and forecast\n",
    "# n_test = 14  # Number of days for testing\n",
    "\n",
    "data_location #Now has only Jacksonville locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode Data\n",
    "label_encoders = {}\n",
    "for column in ['Call Type', 'Priority', 'Nature of Call']:\n",
    "    encoder = LabelEncoder()\n",
    "    data_location[column] = encoder.fit_transform(data_location[column])\n",
    "    label_encoders[column] = encoder\n",
    "\n",
    "#grouped_data = data.groupby(['Date of Service','Location', 'Unit', 'Call Type', 'Priority', 'Nature of Call']).size().reset_index(name='Daily Count')\n",
    "#grouped_data = grouped_data.sort_values(by='Date of Service')\n",
    "\n",
    "#Scaling exogenous values\n",
    "features_to_normalize = ['Call Type', 'Priority', 'Nature of Call']\n",
    "scaler = MinMaxScaler() #0 Values to represent Unit not being there for that day. \n",
    "data_location[features_to_normalize] = scaler.fit_transform(data_location[features_to_normalize]) \n",
    "data_location\n",
    "#grouped_data[features_to_normalize] = scaler.fit_transform(grouped_data[features_to_normalize])\n",
    "#grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e5a08",
   "metadata": {},
   "source": [
    "The cell below uses a grid search to pick the best parameters for SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for SARIMAX\n",
    "p = d = q = range(0, 3)  # Example ranges for ARIMA orders\n",
    "P = D = Q = range(0, 3)\n",
    "s = [7, 14, 30]  #Testing for Weekly, Bi-Weely, and Monthly Seasonality\n",
    "\n",
    "# Random Grid Search across all units\n",
    "parameters = list(itertools.product(p, d, q, P, D, Q, s))\n",
    "pdq_combinations = random.sample(parameters, min(len(parameters), 5))  # Randomly sample combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdq_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdq_combinations = [(1, 0, 2, 2, 1, 1, 7), ] #Testing specific parameters. \n",
    "pdq_combinations.extend(((0, 0, 0, 1, 1, 1, 14),))\n",
    "pdq_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c9074",
   "metadata": {},
   "source": [
    "Section: Used for testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply best parameters for each unit to test\n",
    "# unit_IDs = sorted(data_location['Unit'].unique())\n",
    "\n",
    "# #Splitting training and testing by date\n",
    "# split_date = pd.Timestamp('2023-11-30')\n",
    "\n",
    "# start_date = data_location.index.min()\n",
    "# end_date = data_location.index.max()\n",
    "# date_range = pd.date_range(start= start_date, end= end_date, freq = 'D')\n",
    "# exog_data = data_location[['Call Type', 'Priority', 'Nature of Call']].groupby(data_location.index).mean().reindex(\n",
    "#                                                                                                             date_range, \n",
    "#                                                                                                             fill_value = 0)\n",
    "\n",
    "# for unit_ID in unit_IDs:\n",
    "#     print(f\"Processing Medic Unit: {unit_ID}\")\n",
    "\n",
    "#     unit_data_grouped = data_location[data_location['Unit'] == unit_ID].resample('D').count().reindex(date_range, \n",
    "#                                                                                                       fill_value = 0)\n",
    "#     unit_data_grouped = unit_data_grouped.drop('Location', axis = 1)\n",
    "#     #print(unit_data_grouped['Unit'][0])\n",
    "\n",
    "#     for i in range(len(unit_data_grouped)):\n",
    "#         if unit_data_grouped['Unit'][i] > 0:\n",
    "#             unit_data_grouped['Call Type'][i] = exog_data['Call Type'][i]\n",
    "#             unit_data_grouped['Priority'][i] = exog_data['Priority'][i]\n",
    "#             unit_data_grouped['Nature of Call'][i] = exog_data['Nature of Call'][i]\n",
    "            \n",
    "#             data_unit = unit_data_grouped['Unit']\n",
    "#             exog_unit = unit_data_grouped[['Call Type', 'Priority', 'Nature of Call']]\n",
    "            \n",
    "#     #print(exog_unit)\n",
    "    \n",
    "#     #train_end = pd.to_datetime(\"11/30/2023\")\n",
    "#     train = data_unit[data_unit.index < split_date]\n",
    "#     test = data_unit[data_unit.index >= split_date]\n",
    "\n",
    "#     train_exog = exog_unit[exog_unit.index < split_date]\n",
    "#     test_exog = exog_unit[exog_unit.index >= split_date]\n",
    "    \n",
    "#     print(data_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae571d8",
   "metadata": {},
   "source": [
    "Section: Code Testing - End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax_grid_search(data_location, pdq_combinations): #Aggregated order and seasonal order. \n",
    "#     best_aic = float(\"inf\")\n",
    "#     best_configuration = None\n",
    "#     best_model = None\n",
    "    \n",
    "    # Apply best parameters for each unit to test\n",
    "    unit_IDs = sorted(data_location['Unit'].unique())\n",
    "    \n",
    "    #Used for training/test split further below. \n",
    "    split_date = pd.Timestamp('2023-11-30')\n",
    "    \n",
    "    #Preparing exogenous values\n",
    "    start_date = data_location.index.min()\n",
    "    end_date = data_location.index.max()\n",
    "    date_range = pd.date_range(start= start_date, end= end_date, freq = 'D')\n",
    "    exog_data = data_location[['Call Type', 'Priority', 'Nature of Call']].groupby(data_location.index).mean().reindex(\n",
    "                                                                                                                date_range, \n",
    "                                                                                                                fill_value = 0)\n",
    "    # Initialize a dictionary to store best_configuration\n",
    "    unit_results={}\n",
    "    \n",
    "    for unit_ID in unit_IDs:\n",
    "        best_aic = float(\"inf\")\n",
    "        best_configuration = None\n",
    "        best_model = None\n",
    "        \n",
    "        print(f\"Processing Medic Unit: {unit_ID}\")\n",
    "        \n",
    "        #Using date_range from preparing exogenous values\n",
    "        unit_data_grouped = data_location[data_location['Unit'] == unit_ID].resample('D').count().reindex(date_range, \n",
    "                                                                                                          fill_value = 0)\n",
    "        unit_data_grouped = unit_data_grouped.drop('Location', axis = 1)\n",
    "        #print(unit_data_grouped['Unit'][0])\n",
    "\n",
    "        for i in range(len(unit_data_grouped)):\n",
    "            if unit_data_grouped['Unit'][i] > 0:\n",
    "                unit_data_grouped['Call Type'][i] = exog_data['Call Type'][i]\n",
    "                unit_data_grouped['Priority'][i] = exog_data['Priority'][i]\n",
    "                unit_data_grouped['Nature of Call'][i] = exog_data['Nature of Call'][i]\n",
    "        \n",
    "        data_unit = unit_data_grouped['Unit']\n",
    "        exog_unit = unit_data_grouped[['Call Type', 'Priority', 'Nature of Call']]\n",
    "        \n",
    "        #train_end = pd.to_datetime(\"11/30/2023\")\n",
    "        train = data_unit[data_unit.index < split_date]\n",
    "        test = data_unit[data_unit.index >= split_date]\n",
    "        \n",
    "        train_exog = exog_unit[exog_unit.index < split_date]\n",
    "        test_exog = exog_unit[exog_unit.index >= split_date]\n",
    "        \n",
    "        #For lesser computers.\n",
    "        for pdq in pdq_combinations:\n",
    "            try:\n",
    "                model = SARIMAX(train, order=pdq[:3], seasonal_order=pdq[3:], exog = train_exog, \n",
    "                                enforce_stationarity=False, enforce_invertibility=False)\n",
    "                model_fit = model.fit(disp=False)\n",
    "\n",
    "                if model_fit.aic < best_aic: \n",
    "                    best_aic = model_fit.aic\n",
    "                    best_configuration = pdq\n",
    "                    best_model = model_fit\n",
    "\n",
    "            except Exception as e: \n",
    "                print(f'Exception: {e}')\n",
    "                continue\n",
    "        \n",
    "        #Storing best_configuration\n",
    "        unit_results[unit_ID] = {'best_configuration' : best_configuration}\n",
    "        \n",
    "        if best_configuration is None:\n",
    "            raise ValueError(\"No suitable model parameters found for aggregate data\")\n",
    "            \n",
    "        #Predicting the rest of the forecast to compare to test data. Steps has to be the remaining days to predict. \n",
    "        predictions = best_model.forecast(steps=len(test), exog = test_exog)\n",
    "#         predictions = best_model.get_forecast(steps=len(test), exog = test_exog)\n",
    "#         forecast_values = predictions.predict_mean\n",
    "\n",
    "        # Performance Metrics\n",
    "        mse = mean_squared_error(test, predictions)\n",
    "        mae = mean_absolute_error(test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f'Unit {unit_ID} - Best SARIMAX: {best_configuration} - MSE: {mse}, MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "        # Plotting Forecast vs Actual\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(test, label='Actual')\n",
    "        plt.plot(predictions, label='Forecast', color='red')\n",
    "        plt.title(f'SARIMAX Model Forecast vs Actual for Unit {unit_ID}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Unit')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "    return best_aic, best_configuration, best_model, unit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running Grid Search\n",
    "# #SARIMAX can internally difference with value (d). We manually difference (d=1) just to check stationarity. \n",
    "# #Now we will let SARIMAX integrate difference by setting d=1 manually on the ORIGINAL dataset. \n",
    "\n",
    "#ts_data = df['Actual Trips']\n",
    "#ts_data = train['Unit']\n",
    "\n",
    "ts_data = data_location\n",
    "\n",
    "# exog_train = train[['Call Type', 'Priority', 'Nature of Call']]\n",
    "# exog_test = test[['Call Type', 'Priority', 'Nature of Call']]\n",
    "\n",
    "# #For lesser PC\n",
    "best_aic, best_configuration, best_model, unit_results = sarimax_grid_search(ts_data, pdq_combinations)\n",
    "\n",
    "#print(f\"Best SARIMAX parameters: {best_configuration} with AIC: {best_aic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71902824",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configuration #You can interupt cell above to get the current best_params value without having to finish all the calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = unit_results['MEDIC 203'].values()\n",
    "results = list(best_config)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126079f1",
   "metadata": {},
   "source": [
    "Everything below is to generate future values. (Needs to be reworked.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e556d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The best parameters for all Units combined; Unit MEDIC 1 usually has highest RSME so minimizing this\n",
    "    #Best SARIMAX: (0, 0, 0, 2, 0, 1, 7) - MSE: 3.477645430848429, MAE: 1.3565603776833444, RMSE: 1.8648446130571\n",
    "\n",
    "unique_locations = data['Location'].unique()\n",
    "print(\"Unique Locations:\", unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the filtering condition based on the actual string in your dataset\n",
    "data_location = data[data['Location'] == 'PONTE VEDRA BEACH']\n",
    "\n",
    "# Get unique 'Medic Units' \n",
    "unit_IDs = data_location['Unit'].unique()\n",
    "unit_IDs = sorted(unit_IDs)\n",
    "print(\"Unique Units in Ponte Vedra Beach:\", unit_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85445485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Vehicle IDs in Ponte Vedra Beach:\", unit_IDs)\n",
    "\n",
    "#Preparing exogenous values\n",
    "#Using date_range from preparing exogenous values\n",
    "start_date = data_location.index.min()\n",
    "end_date = data_location.index.max()\n",
    "date_range = pd.date_range(start= start_date, end= end_date, freq = 'D')\n",
    "\n",
    "models = {} #Fitting model for all units individually.\n",
    "for unit_ID in unit_IDs:\n",
    "    print(f\"Processing Call Type: {unit_ID}\")\n",
    "\n",
    "    # Aggregate data by day for each 'Unit'\n",
    "    ts = data_location[data_location['Unit'] == unit_ID].resample('D').count().reindex(date_range, fill_value = 0)\n",
    "    ts = ts.drop('Location', axis = 1)\n",
    "    ts_data = ts['Unit']\n",
    "    \n",
    "    #print(f\"Sample Data for {unit_ID}:\\n\", ts.head())  # Print sample data\n",
    "    #Retrieve best_configuration for current unit\n",
    "    results = unit_results[unit_ID].values()\n",
    "    results = list(results)\n",
    "    best_configuration = results[0]\n",
    "    \n",
    "    # Check if time series is not empty\n",
    "    if not ts.empty:\n",
    "        model = SARIMAX(ts_data, order=best_configuration[:3], seasonal_order=best_configuration[3:], #exog = train_exog, #Implementing later\n",
    "                       enforce_stationarity=False, enforce_invertibility=False) \n",
    "        models[unit_ID] = model.fit(disp=True)\n",
    "    else:\n",
    "        print(f\"No data available for Call Type: {unit_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = {}\n",
    "steps = 14\n",
    "for unit_ID, model in models.items():\n",
    "    forecasts[unit_ID] = model.forecast(steps)  # Forecasting for the next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fe2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Determine the date for the forecast\n",
    "# Assuming you are forecasting for the next day after the last date in your data\n",
    "last_date = data.index.max()\n",
    "start_forecast_date = last_date + timedelta(days=1)\n",
    "\n",
    "# Initialize a dictionary to store forecasts for each date\n",
    "date_forecasts = {start_forecast_date + timedelta(days=i): [] for i in range(steps)}\n",
    "\n",
    "# Generate and store forecasts for each call type and each date\n",
    "for unit_ID in unit_IDs:\n",
    "    model = models.get(unit_ID)\n",
    "    if model:\n",
    "        forecast_values = model.forecast(steps=steps)\n",
    "        forecast_values = abs(forecast_values)\n",
    "        for i in range(steps):\n",
    "            forecast_date = start_forecast_date + timedelta(days=i)\n",
    "            # Append the forecast string for each call type and date\n",
    "            #date_forecasts[forecast_date].append(f\"{forecast_values[i]:.0f} calls of type '{vehicle_ID}'\")\n",
    "            #date_forecasts[forecast_date].append(f\"{vehicle_ID}: {forecast_values[i]:.0f}\")\n",
    "            date_forecasts[forecast_date].append(f\"{unit_ID}: {forecast_values[i]:.0f}\")\n",
    "    else:\n",
    "        # Handle case where there is no model for a call type\n",
    "        for date in date_forecasts:\n",
    "            date_forecasts[date].append(f\"0 calls for unit '{unit_ID}'\")\n",
    "\n",
    "# Print the combined forecast message for each date\n",
    "for date, forecasts in date_forecasts.items():\n",
    "    forecast_message = \" \\n\".join(forecasts)\n",
    "    print(f\"There will be: \\n{forecast_message}\\nin the location of Ponte Vedra Beach{date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e178c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating an empty DataFrame with specified columns\n",
    "#forecast_results_df = pd.DataFrame(columns=['Date of Service', 'Call Type & Volume', 'Location'])\n",
    "\n",
    "# Initialize a list to store forecast data for DataFrame\n",
    "forecast_data = []\n",
    "\n",
    "#Generate forecast data suitable for DataFrame\n",
    "for date, forecasts in date_forecasts.items():\n",
    "    #forecast_message = \" and \".join(forecasts)\n",
    "    forecast_message = ' , '.join(forecasts)\n",
    "    forecast_data.append({\n",
    "        \"Date of Service\": date.strftime('%A: %m-%d-%Y'),\n",
    "        \"Unit\": forecast_message,\n",
    "        \"Location\": \"Ponte Vedra Beach\"  # or dynamically set location if needed\n",
    "    })\n",
    "    \n",
    "# Create a DataFrame\n",
    "forecast_df = pd.DataFrame(forecast_data)\n",
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_file_path = 'Model Forecast Results.csv'\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "try:\n",
    "    forecast_df.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    success = False\n",
    "    error_message = str(e)\n",
    "\n",
    "# Returning the path if successful, else the error message\n",
    "output_file_path if success else error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cd3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
